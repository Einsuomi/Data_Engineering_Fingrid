{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7caae4a2-9bb6-40f4-b9e8-285667463b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# fingrid_test_workspace.fingrid_silver.solar_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833d7b22-933f-4eeb-a124-c714099bbb06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from delta.tables import *\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "catalog = \"fingrid_test_workspace\"\n",
    "\n",
    "# raw data\n",
    "bronze_schema = \"fingrid_bronze\"\n",
    "raw_table_name = \"solar_forecast_raw\"\n",
    "raw_table = \".\".join([catalog, bronze_schema, raw_table_name])\n",
    "\n",
    "\n",
    "# silver data \n",
    "silver_schema = \"fingrid_silver\"\n",
    "silver_table_name = \"solar_forecast\"\n",
    "silver_table = \".\".join([catalog, silver_schema, silver_table_name])\n",
    "\n",
    "# control table \n",
    "control_table = \"fingrid_test_workspace.fingrid_load_control.load_control\"\n",
    "\n",
    "\n",
    "local_tz = pytz.timezone(\"Europe/Helsinki\")  \n",
    "\n",
    "if spark.catalog.tableExists(control_table):\n",
    "    current_bronze_ingestion_timestamp = spark.read.table(control_table).where(F.col(\"source_dataset_id\") =='248').select(F.max(\"bronze_ingestion_timestamp\")).collect()[0][0]\n",
    "\n",
    "    # current_bronze_ingestion_timestamp = \"2025-01-01T00:00:00.000+00:00\"\n",
    "\n",
    "    print(\"current_bronze_ingestion_timestamp:\",current_bronze_ingestion_timestamp)\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(raw_table)\n",
    "        .where((F.col(\"ingestion_timestamp\") > current_bronze_ingestion_timestamp ) )\n",
    "    )\n",
    "\n",
    "        \n",
    "    print(\"Total rows(pages): \", df.count())\n",
    "    print(\"read data done\", datetime.now(tz=local_tz))\n",
    "\n",
    "else:\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(raw_table)\n",
    "    )\n",
    "if df.isEmpty():\n",
    "        print (\"No data\")\n",
    "else:\n",
    "    max_bronze_ingestion_timestamp = df.select(F.max(F.col(\"ingestion_timestamp\"))).collect()[0][0]\n",
    "    print(\"max_bronze_ingestion_timestamp:\",max_bronze_ingestion_timestamp)\n",
    "\n",
    "\n",
    "    print (\"start to transform data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "\n",
    "    df_flat = df.select(\n",
    "        F.explode(\n",
    "            F.from_json(\n",
    "                F.col(\"data\"),\n",
    "                \"ARRAY<STRUCT<datasetId: STRING, startTime: TIMESTAMP, endTime: TIMESTAMP, value: DOUBLE>>\"\n",
    "            )\n",
    "        ).alias(\"record\")\n",
    "    )\n",
    "    df_result = df_flat.select(\n",
    "        F.col(\"record.datasetId\").alias(\"dataset_id\"),\n",
    "        F.col(\"record.startTime\").alias(\"start_time\"),\n",
    "        F.col(\"record.endTime\").alias(\"end_time\"),\n",
    "        F.col(\"record.value\").alias(\"value\")\n",
    "    )\n",
    "\n",
    "    df_result = df_result.withColumn(\"refresh_timestamp\", F.current_timestamp())\n",
    "\n",
    "\n",
    "    print (\"start to insert or update data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "    # update data to table\n",
    "    df_existing_silver_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=silver_table)\n",
    "    df_control_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=control_table)\n",
    "    df_existing_silver_table.alias('df_existing') \\\n",
    "        .merge(\n",
    "            df_result.alias('updates'),\n",
    "            \"df_existing.dataset_id = updates.dataset_id and df_existing.start_time = updates.start_time and df_existing.end_time = updates.end_time\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"value\": \"updates.value\",\n",
    "            \"refresh_timestamp\": \"updates.refresh_timestamp\"\n",
    "        }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "    df_control_table.alias('control_table').update(\n",
    "        condition=F.col('source_dataset_id') == '248',\n",
    "        set={'bronze_ingestion_timestamp': F.to_timestamp(F.lit(max_bronze_ingestion_timestamp))}\n",
    "    )\n",
    "   \n",
    "    print(\"update bronze_ingestion_timestamp:\",max_bronze_ingestion_timestamp)\n",
    "    print (\"insert and update done: \", datetime.now(tz=local_tz))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4e3dd8f-053f-4dc3-b697-0cd609d0d31d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#  fingrid_test_workspace.fingrid_silver.wind_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ab5616-c968-4ec5-b393-34b816e2864a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from delta.tables import *\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "catalog = \"fingrid_test_workspace\"\n",
    "\n",
    "# raw data\n",
    "bronze_schema = \"fingrid_bronze\"\n",
    "raw_table_name = \"wind_forecast_raw\"\n",
    "raw_table = \".\".join([catalog, bronze_schema, raw_table_name])\n",
    "\n",
    "\n",
    "# silver data \n",
    "silver_schema = \"fingrid_silver\"\n",
    "silver_table_name = \"wind_forecast\"\n",
    "silver_table = \".\".join([catalog, silver_schema, silver_table_name])\n",
    "\n",
    "# control table \n",
    "control_table = \"fingrid_test_workspace.fingrid_load_control.load_control\"\n",
    "\n",
    "\n",
    "local_tz = pytz.timezone(\"Europe/Helsinki\")  \n",
    "\n",
    "if spark.catalog.tableExists(control_table):\n",
    "    current_bronze_ingestion_timestamp = spark.read.table(control_table).where(F.col(\"source_dataset_id\") =='245').select(F.max(\"bronze_ingestion_timestamp\")).collect()[0][0]\n",
    "\n",
    "    # current_bronze_ingestion_timestamp = \"2025-01-01T00:00:00.000+00:00\"\n",
    "\n",
    "    print(\"current_bronze_ingestion_timestamp:\",current_bronze_ingestion_timestamp)\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(raw_table)\n",
    "        .where((F.col(\"ingestion_timestamp\") > current_bronze_ingestion_timestamp ) )\n",
    "    )\n",
    "\n",
    "        \n",
    "    print(\"Total rows(pages): \", df.count())\n",
    "    print(\"read data done\", datetime.now(tz=local_tz))\n",
    "\n",
    "else:\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(raw_table)\n",
    "    )\n",
    "if df.isEmpty():\n",
    "        print (\"No data\")\n",
    "else:\n",
    "    max_bronze_ingestion_timestamp = df.select(F.max(F.col(\"ingestion_timestamp\"))).collect()[0][0]\n",
    "    print(\"max_bronze_ingestion_timestamp:\",max_bronze_ingestion_timestamp)\n",
    "\n",
    "\n",
    "    print (\"start to transform data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "\n",
    "    df_flat = df.select(\n",
    "        F.explode(\n",
    "            F.from_json(\n",
    "                F.col(\"data\"),\n",
    "                \"ARRAY<STRUCT<datasetId: STRING, startTime: TIMESTAMP, endTime: TIMESTAMP, value: DOUBLE>>\"\n",
    "            )\n",
    "        ).alias(\"record\")\n",
    "    )\n",
    "    df_result = df_flat.select(\n",
    "        F.col(\"record.datasetId\").alias(\"dataset_id\"),\n",
    "        F.col(\"record.startTime\").alias(\"start_time\"),\n",
    "        F.col(\"record.endTime\").alias(\"end_time\"),\n",
    "        F.col(\"record.value\").alias(\"value\")\n",
    "    )\n",
    "\n",
    "    df_result = df_result.withColumn(\"refresh_timestamp\", F.current_timestamp())\n",
    "\n",
    "\n",
    "    print (\"start to insert or update data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "    # update data to table\n",
    "    df_existing_silver_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=silver_table)\n",
    "    df_control_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=control_table)\n",
    "    df_existing_silver_table.alias('df_existing') \\\n",
    "        .merge(\n",
    "            df_result.alias('updates'),\n",
    "            \"df_existing.dataset_id = updates.dataset_id and df_existing.start_time = updates.start_time and df_existing.end_time = updates.end_time\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"value\": \"updates.value\",\n",
    "            \"refresh_timestamp\": \"updates.refresh_timestamp\"\n",
    "        }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "    df_control_table.alias('control_table').update(\n",
    "        condition=F.col('source_dataset_id') == '245',\n",
    "        set={'bronze_ingestion_timestamp': F.to_timestamp(F.lit(max_bronze_ingestion_timestamp))}\n",
    "    )\n",
    "   \n",
    "    print(\"update bronze_ingestion_timestamp:\",max_bronze_ingestion_timestamp)\n",
    "    print (\"insert and update done: \", datetime.now(tz=local_tz))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44d00363-6f4c-47d0-a2cd-4c2b4e171674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7645029e-2c64-4564-8628-65bf87bfb29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the full schema including additionalJson\n",
    "schema = \"\"\"\n",
    "ARRAY<STRUCT<\n",
    "    datasetId: STRING,\n",
    "    startTime: TIMESTAMP,\n",
    "    endTime: TIMESTAMP,\n",
    "    value: DOUBLE,\n",
    "    additionalJson: STRUCT<\n",
    "        Count: STRING,\n",
    "        CustomerType: STRING,\n",
    "        ReadTS: TIMESTAMP,\n",
    "        Res: STRING,\n",
    "        TimeSeriesType: STRING,\n",
    "        Uom: STRING,\n",
    "        Value: STRING\n",
    "    >\n",
    ">>\n",
    "\"\"\"\n",
    "\n",
    "# Explode the array with proper schema\n",
    "df_flat = df.select(\n",
    "    F.explode(F.from_json(F.col(\"data\"), schema)).alias(\"record\")\n",
    ")\n",
    "\n",
    "# Extract all fields including nested ones\n",
    "df_result = df_flat.select(\n",
    "    F.col(\"record.datasetId\").alias(\"dataset_id\"),\n",
    "    F.col(\"record.startTime\").alias(\"start_time\"),\n",
    "    F.col(\"record.endTime\").alias(\"end_time\"),\n",
    "    F.col(\"record.value\").alias(\"value\"),\n",
    "    F.col(\"record.additionalJson.Count\").alias(\"count\"),\n",
    "    F.col(\"record.additionalJson.CustomerType\").alias(\"customer_type\"),\n",
    "    F.col(\"record.additionalJson.ReadTS\").alias(\"read_ts\"),\n",
    "    F.col(\"record.additionalJson.Res\").alias(\"res\"),\n",
    "    F.col(\"record.additionalJson.TimeSeriesType\").alias(\"time_series_type\"),\n",
    "    F.col(\"record.additionalJson.Uom\").alias(\"uom\"),\n",
    "    F.col(\"record.additionalJson.Value\").alias(\"additional_value\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00549994-c2a0-4719-b7e2-c0bfc9f1d17c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from delta.tables import *\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "catalog = \"fingrid_test_workspace\"\n",
    "\n",
    "# raw data\n",
    "bronze_schema = \"fingrid_bronze\"\n",
    "raw_table_name = \"electricity_consumption_raw\"\n",
    "raw_table = \".\".join([catalog, bronze_schema, raw_table_name])\n",
    "\n",
    "\n",
    "# silver data \n",
    "silver_schema = \"fingrid_silver\"\n",
    "silver_table_name = \"electricity_consumption\"\n",
    "silver_table = \".\".join([catalog, silver_schema, silver_table_name])\n",
    "\n",
    "# control table \n",
    "control_table = \"fingrid_test_workspace.fingrid_load_control.load_control\"\n",
    "\n",
    "\n",
    "local_tz = pytz.timezone(\"Europe/Helsinki\")  \n",
    "\n",
    "if spark.catalog.tableExists(control_table):\n",
    "    current_bronze_ingestion_timestamp = spark.read.table(control_table).where(F.col(\"source_dataset_id\") =='358').select(F.max(\"bronze_ingestion_timestamp\")).collect()[0][0]\n",
    "\n",
    "    # current_bronze_ingestion_timestamp = \"2025-01-01T00:00:00.000+00:00\"\n",
    "\n",
    "    print(\"current_bronze_ingestion_timestamp:\",current_bronze_ingestion_timestamp)\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(raw_table)\n",
    "        .where((F.col(\"ingestion_timestamp\") > current_bronze_ingestion_timestamp ) )\n",
    "    )\n",
    "\n",
    "        \n",
    "    print(\"Total rows(pages): \", df.count())\n",
    "    print(\"read data done\", datetime.now(tz=local_tz))\n",
    "\n",
    "else:\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(raw_table)\n",
    "    )\n",
    "if df.isEmpty():\n",
    "        print (\"No data\")\n",
    "else:\n",
    "    max_bronze_ingestion_timestamp = df.select(F.max(F.col(\"ingestion_timestamp\"))).collect()[0][0]\n",
    "    print(\"max_bronze_ingestion_timestamp:\",max_bronze_ingestion_timestamp)\n",
    "\n",
    "\n",
    "    print (\"start to transform data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "    # Define the full schema including additionalJson\n",
    "    schema = \"\"\"\n",
    "    ARRAY<STRUCT<\n",
    "        datasetId: STRING,\n",
    "        startTime: TIMESTAMP,\n",
    "        endTime: TIMESTAMP,\n",
    "        value: DOUBLE,\n",
    "        additionalJson: STRUCT<\n",
    "            Count: STRING,\n",
    "            CustomerType: STRING,\n",
    "            ReadTS: TIMESTAMP,\n",
    "            Res: STRING,\n",
    "            TimeSeriesType: STRING,\n",
    "            Uom: STRING,\n",
    "            Value: STRING\n",
    "        >\n",
    "    >>\n",
    "    \"\"\"\n",
    "\n",
    "    # Explode the array with proper schema\n",
    "    df_flat = df.select(\n",
    "        F.explode(F.from_json(F.col(\"data\"), schema)).alias(\"record\")\n",
    "    )\n",
    "\n",
    "    # Extract all fields including nested ones\n",
    "    df_result = df_flat.select(\n",
    "        F.col(\"record.datasetId\").alias(\"dataset_id\"),\n",
    "        F.col(\"record.startTime\").alias(\"start_time\"),\n",
    "        F.col(\"record.endTime\").alias(\"end_time\"),\n",
    "        F.col(\"record.value\").alias(\"value\"),\n",
    "        F.col(\"record.additionalJson.Count\").alias(\"count\"),\n",
    "        F.col(\"record.additionalJson.CustomerType\").alias(\"customer_type\"),\n",
    "        F.col(\"record.additionalJson.ReadTS\").alias(\"read_ts\"),\n",
    "        F.col(\"record.additionalJson.Res\").alias(\"res\"),\n",
    "        F.col(\"record.additionalJson.TimeSeriesType\").alias(\"time_series_type\"),\n",
    "        F.col(\"record.additionalJson.Uom\").alias(\"uom\"),\n",
    "        F.col(\"record.additionalJson.Value\").alias(\"additional_value\")\n",
    "    )\n",
    "\n",
    "    df_result = df_result.withColumn(\"refresh_timestamp\", F.current_timestamp())\n",
    "\n",
    "\n",
    "    print (\"start to insert or update data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "    # update data to table\n",
    "    df_existing_silver_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=silver_table)\n",
    "    df_control_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=control_table)\n",
    "    df_existing_silver_table.alias('df_existing') \\\n",
    "        .merge(\n",
    "            df_result.alias('updates'),\n",
    "            \"df_existing.dataset_id = updates.dataset_id and df_existing.start_time = updates.start_time and df_existing.end_time = updates.end_time and df_existing.customer_type = updates.customer_type\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "    df_control_table.alias('control_table').update(\n",
    "        condition=F.col('source_dataset_id') == '358',\n",
    "        set={'bronze_ingestion_timestamp': F.to_timestamp(F.lit(max_bronze_ingestion_timestamp))}\n",
    "    )\n",
    "   \n",
    "    print(\"update bronze_ingestion_timestamp:\",max_bronze_ingestion_timestamp)\n",
    "    print (\"insert and update done: \", datetime.now(tz=local_tz))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ff0de05-ced9-4bee-8f96-656d62346995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8783563809290846,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2. read bronze data and write to silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
