{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c247ca92-a795-41bd-b4a8-42a06da578a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import row_number, lit\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import *\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "catalog = \"fingrid_test_workspace\"\n",
    "\n",
    "silver_schema = \"fingrid_silver\"\n",
    "gold_schema = \"fingrid_gold\"\n",
    "\n",
    "table_name_silver = \"electricity_consumption\"\n",
    "silver_table = \".\".join([catalog, silver_schema, table_name_silver])\n",
    "\n",
    "# control table \n",
    "control_table = \"fingrid_test_workspace.fingrid_load_control.load_control\"\n",
    "\n",
    "# gold table \n",
    "gold_table_name = \"fact_consumption\"\n",
    "gold_table = \".\".join([catalog, gold_schema, gold_table_name])\n",
    "\n",
    "\n",
    "# dimdate\n",
    "date_table_name = \"dim_date\"\n",
    "table_date = \".\".join([catalog, gold_schema, date_table_name])\n",
    "\n",
    "# dimtime\n",
    "time_table_name = \"dim_time\"\n",
    "table_time = \".\".join([catalog, gold_schema, time_table_name])\n",
    "\n",
    "\n",
    "local_tz = pytz.timezone(\"Europe/Helsinki\")  \n",
    "\n",
    "if spark.catalog.tableExists(control_table):\n",
    "    current_silver_refresh_timestamp = spark.read.table(control_table).where(F.col(\"source_dataset_id\") =='358').select(F.max(\"silver_refresh_timestamp\")).collect()[0][0]\n",
    "\n",
    "    # current_silver_refresh_timestamp = \"2025-01-01T00:00:00.000+00:00\"\n",
    "\n",
    "    print(\"current_silver_refresh_timestamp:\",current_silver_refresh_timestamp)\n",
    "\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(silver_table)\n",
    "        .where((F.col(\"refresh_timestamp\") > current_silver_refresh_timestamp ) )\n",
    "    )\n",
    "\n",
    "        \n",
    "    print(\"Total rows(pages): \", df.count())\n",
    "    print(\"read data done\", datetime.now(tz=local_tz))\n",
    "\n",
    "else:\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(silver_table)\n",
    "    )\n",
    "if df.isEmpty():\n",
    "        print (\"No data\")\n",
    "else:\n",
    "    max_silver_refresh_timestamp = df.select(F.max(F.col(\"refresh_timestamp\"))).collect()[0][0]\n",
    "    print(\"max_silver_refresh_timestamp:\",max_silver_refresh_timestamp)\n",
    "\n",
    "\n",
    "    print (\"start to transform data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "    ### start transform data \n",
    "\n",
    "    df = df.drop(\"refresh_timestamp\", \"dataset_id\", \"uom\", \"read_ts\")\n",
    "\n",
    "\n",
    "    df = df.withColumn(\"start_date\", F.col(\"start_time\").cast(\"date\"))\n",
    "    df = df.withColumn(\"end_date\", F.col(\"end_time\").cast(\"date\"))\n",
    "\n",
    "    df = df.withColumn(\n",
    "    \"start_time\",\n",
    "    F.when(\n",
    "        F.col(\"start_time\").isNotNull(),\n",
    "        F.regexp_extract(F.col(\"start_time\"), r\"\\d{2}:\\d{2}:\\d{2}\", 0)\n",
    "    ).otherwise(None)\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"end_time\",\n",
    "        F.when(\n",
    "            F.col(\"end_time\").isNotNull(),\n",
    "            F.regexp_extract(F.col(\"end_time\"), r\"\\d{2}:\\d{2}:\\d{2}\", 0)\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "\n",
    "    # get start date id \n",
    "    df_date = spark.read.format(\"delta\").table(table_date)\n",
    "    join =    ((F.col(\"f.start_date\") == F.col(\"d.date\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_date.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.date_id\").alias(\"start_date_id\"),        \n",
    "    ).drop(\"start_date\")\n",
    "    )\n",
    "\n",
    "    # get end date_id\n",
    "    join =    ((F.col(\"f.end_date\") == F.col(\"d.date\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_date.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.date_id\").alias(\"end_date_id\"),        \n",
    "    ).drop(\"end_date\")\n",
    "    )\n",
    "\n",
    "    # get start time id \n",
    "    df_time = spark.read.format(\"delta\").table(table_time)\n",
    "    join =    ((F.col(\"f.start_time\") == F.col(\"d.time_15min\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_time.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.time_quarter_id\").alias(\"start_time_id\"),        \n",
    "    ).drop(\"start_time\")\n",
    "    )\n",
    "\n",
    "    # get end time_id\n",
    "    join =    ((F.col(\"f.end_time\") == F.col(\"d.time_15min\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_time.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.time_quarter_id\").alias(\"end_time_id\"),        \n",
    "    ).drop(\"end_time\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    customer_table_name = \"dim_customer\"\n",
    "    table_customer = \".\".join([catalog, gold_schema, customer_table_name])\n",
    "\n",
    "    df_customer = spark.read.format(\"delta\").table(table_customer)\n",
    "\n",
    "    # get customerID\n",
    "    join =    ((F.col(\"f.customer_type\") == F.col(\"d.customer_type\")) & \n",
    "               (F.col(\"f.time_series_type\") == F.col(\"d.time_series_type\")) &\n",
    "               (F.col(\"f.res\") == F.col(\"d.res\"))\n",
    "               )       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_customer.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.customerID\").alias(\"customerID\"),        \n",
    "    ).drop(\"customer_type\", \"time_series_type\",\"res\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"additional_value\", F.col(\"additional_value\").cast(\"double\"))\n",
    "    df = df.withColumn(\"count\", F.col(\"count\").cast(\"bigint\"))\n",
    "\n",
    "    df = df.withColumn(\"refresh_timestamp\", F.current_timestamp())\n",
    "\n",
    "\n",
    "    # update data to table\n",
    "    df_existing_gold_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=gold_table)\n",
    "    df_control_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=control_table)\n",
    "    df_existing_gold_table.alias('df_existing') \\\n",
    "        .merge(\n",
    "            df.alias('updates'),\n",
    "            \"df_existing.start_date_id = updates.start_date_id and df_existing.end_date_id = updates.end_date_id and df_existing.start_time_id = updates.start_time_id and df_existing.end_time_id = updates.end_time_id and df_existing.customerID = updates.customerID\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "    df_control_table.alias('control_table').update(\n",
    "        condition=F.col('source_dataset_id') == '358',\n",
    "        set={'silver_refresh_timestamp': F.to_timestamp(F.lit(max_silver_refresh_timestamp))}\n",
    "    )\n",
    "   \n",
    "    print(\"update max_silver_refresh_timestamp: \",max_silver_refresh_timestamp)\n",
    "    print (\"insert and update done: \", datetime.now(tz=local_tz))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 180469345406132,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fact_consumption",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
