{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e27fa25-2acb-4c92-977c-c04c0263f8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import row_number, lit\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import *\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "\n",
    "catalog = \"fingrid_test_workspace\"\n",
    "\n",
    "silver_schema = \"fingrid_silver\"\n",
    "gold_schema = \"fingrid_gold\"\n",
    "\n",
    "table_name_solar = \"solar_forecast\"\n",
    "table_solar = \".\".join([catalog, silver_schema, table_name_solar])\n",
    "\n",
    "table_name_wind = \"wind_forecast\"\n",
    "table_wind = \".\".join([catalog, silver_schema, table_name_wind])\n",
    "\n",
    "date_table_name = \"dim_date\"\n",
    "table_date = \".\".join([catalog, gold_schema, date_table_name])\n",
    "\n",
    "time_table_name = \"dim_time\"\n",
    "table_time = \".\".join([catalog, gold_schema, time_table_name])\n",
    "\n",
    "\n",
    "\n",
    "# control table \n",
    "control_table = \"fingrid_test_workspace.fingrid_load_control.load_control\"\n",
    "\n",
    "# gold table \n",
    "gold_table_name = \"fact_forecast\"\n",
    "gold_table = \".\".join([catalog, gold_schema, gold_table_name])\n",
    "\n",
    "\n",
    "local_tz = pytz.timezone(\"Europe/Helsinki\")  \n",
    "\n",
    "if spark.catalog.tableExists(control_table):\n",
    "    current_solar_silver_refresh_timestamp = spark.read.table(control_table).where(F.col(\"source_dataset_id\") =='248').select(F.max(\"silver_refresh_timestamp\")).collect()[0][0]\n",
    "    current_wind_silver_refresh_timestamp = spark.read.table(control_table).where(F.col(\"source_dataset_id\") =='245').select(F.max(\"silver_refresh_timestamp\")).collect()[0][0]\n",
    "\n",
    "    # current_solar_silver_refresh_timestamp = \"2025-01-01T00:00:00.000+00:00\"\n",
    "    # current_wind_silver_refresh_timestamp = \"2025-01-01T00:00:00.000+00:00\"\n",
    "\n",
    "    print(\"current_solar_silver_refresh_timestamp:\",current_solar_silver_refresh_timestamp)\n",
    "    print(\"current_solar_silver_refresh_timestamp:\",current_wind_silver_refresh_timestamp)\n",
    "\n",
    "\n",
    "    df_solar = spark.read.format(\"delta\").table(table_solar).where(F.col(\"refresh_timestamp\") > current_solar_silver_refresh_timestamp)\n",
    "    print(\"Total rows(pages) of df_solar: \", df_solar.count())\n",
    "    df_wind = spark.read.format(\"delta\").table(table_wind).where(F.col(\"refresh_timestamp\") > current_wind_silver_refresh_timestamp)\n",
    "    print(\"Total rows(pages) of df_wind: \", df_wind.count())\n",
    "\n",
    "    df = df_solar.union(df_wind)\n",
    "    df = df.drop(\"refresh_timestamp\")\n",
    "\n",
    "    print(\"Total rows(pages): \", df.count())\n",
    "    print(\"read data done\", datetime.now(tz=local_tz))\n",
    "\n",
    "else:\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(table_solar)\\\n",
    "        .union(spark.read.format(\"delta\")\n",
    "        .table(table_wind))\n",
    "    )\n",
    "if df.isEmpty():\n",
    "        print (\"No data\")\n",
    "else:\n",
    "    max_solar_silver_refresh_timestamp = df_solar.select(F.max(F.col(\"refresh_timestamp\"))).collect()[0][0]\n",
    "    max_wind_silver_refresh_timestamp = df_wind.select(F.max(F.col(\"refresh_timestamp\"))).collect()[0][0]\n",
    "\n",
    "    print(\"max_solar_silver_refresh_timestamp:\",max_solar_silver_refresh_timestamp)\n",
    "    print(\"max_wind_silver_refresh_timestamp:\",max_wind_silver_refresh_timestamp)\n",
    "\n",
    "    print (\"start to transform data: \", datetime.now(tz=local_tz))\n",
    "\n",
    "    ### start to transform data\n",
    "    df = df.withColumn(\"start_date\", F.col(\"start_time\").cast(\"date\"))\n",
    "    df = df.withColumn(\"end_date\", F.col(\"end_time\").cast(\"date\"))\n",
    "\n",
    "    df = df.withColumn(\"start_time\", F.split(F.col(\"start_time\"), \"T\").getItem(1).substr(1, 8))\n",
    "    df = df.withColumn(\"end_time\", F.split(F.col(\"end_time\"), \"T\").getItem(1).substr(1, 8))\n",
    "\n",
    "    # get start date id \n",
    "    df_date = spark.read.format(\"delta\").table(table_date)\n",
    "    join =    ((F.col(\"f.start_date\") == F.col(\"d.date\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_date.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.date_id\").alias(\"start_date_id\"),        \n",
    "    ).drop(\"start_date\")\n",
    "    )\n",
    "\n",
    "    # get end date_id\n",
    "    join =    ((F.col(\"f.end_date\") == F.col(\"d.date\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_date.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.date_id\").alias(\"end_date_id\"),        \n",
    "    ).drop(\"end_date\")\n",
    "    )\n",
    "\n",
    "    # get start time id \n",
    "    df_time = spark.read.format(\"delta\").table(table_time)\n",
    "    join =    ((F.col(\"f.start_time\") == F.col(\"d.time_15min\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_time.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.time_quarter_id\").alias(\"start_time_id\"),        \n",
    "    ).drop(\"start_time\")\n",
    "    )\n",
    "\n",
    "    # get end date_id\n",
    "    join =    ((F.col(\"f.end_time\") == F.col(\"d.time_15min\")))       \n",
    "    df = (\n",
    "        df.alias(\"f\")\n",
    "            .join(\n",
    "                df_time.alias(\"d\"), join, \"left\"\n",
    "            ).select(\"f.*\", \n",
    "                    F.col(\"d.time_quarter_id\").alias(\"end_time_id\"),        \n",
    "    ).drop(\"end_time\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumnRenamed(\"dataset_id\", \"source_dataset_id\")\n",
    "\n",
    "    df = df.withColumn(\"refresh_timestamp\", F.current_timestamp())\n",
    "\n",
    "    # update data to table\n",
    "    df_existing_gold_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=gold_table)\n",
    "    df_control_table = DeltaTable.forName( sparkSession=spark, tableOrViewName=control_table)\n",
    "    df_existing_gold_table.alias('df_existing') \\\n",
    "        .merge(\n",
    "            df.alias('updates'),\n",
    "            \"df_existing.start_date_id = updates.start_date_id and df_existing.end_date_id = updates.end_date_id and df_existing.start_time_id = updates.start_time_id and df_existing.end_time_id = updates.end_time_id and df_existing.source_dataset_id = updates.source_dataset_id\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdateAll() \\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "    df_control_table.alias('control_table').update(\n",
    "        condition=F.col('source_dataset_id') == '248',\n",
    "        set={'silver_refresh_timestamp': F.to_timestamp(F.lit(max_solar_silver_refresh_timestamp))}\n",
    "    )\n",
    "\n",
    "    df_control_table.alias('control_table').update(\n",
    "    condition=F.col('source_dataset_id') == '245',\n",
    "    set={'silver_refresh_timestamp': F.to_timestamp(F.lit(max_wind_silver_refresh_timestamp))}\n",
    "    )\n",
    "   \n",
    "    print(\"update max_solar_silver_refresh_timestamp: \",max_solar_silver_refresh_timestamp)\n",
    "    print(\"update max_wind_silver_refresh_timestamp: \",max_wind_silver_refresh_timestamp)\n",
    "    print (\"insert and update done: \", datetime.now(tz=local_tz))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 180469345406105,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fact_forecast",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
