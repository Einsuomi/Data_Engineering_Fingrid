{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e27fa25-2acb-4c92-977c-c04c0263f8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import row_number, lit\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "\n",
    "catalog = \"fingrid_test_workspace\"\n",
    "\n",
    "silver_schema = \"fingrid_silver\"\n",
    "gold_schema = \"fingrid_gold\"\n",
    "\n",
    "table_name_solar = \"solar_forecast\"\n",
    "table_solar = \".\".join([catalog, silver_schema, table_name_solar])\n",
    "\n",
    "table_name_wind = \"wind_forecast\"\n",
    "table_wind = \".\".join([catalog, silver_schema, table_name_wind])\n",
    "\n",
    "date_table_name = \"dim_date\"\n",
    "table_date = \".\".join([catalog, gold_schema, date_table_name])\n",
    "\n",
    "time_table_name = \"dim_time\"\n",
    "table_time = \".\".join([catalog, gold_schema, time_table_name])\n",
    "\n",
    "df = spark.read.format(\"delta\").table(table_solar).union( spark.read.format(\"delta\").table(table_wind))\n",
    "df = df.drop(\"refresh_timestamp\")\n",
    "\n",
    "\n",
    "df = df.withColumn(\"start_date\", F.col(\"start_time\").cast(\"date\"))\n",
    "df = df.withColumn(\"end_date\", F.col(\"end_time\").cast(\"date\"))\n",
    "\n",
    "df = df.withColumn(\"start_time\", F.split(F.col(\"start_time\"), \"T\").getItem(1).substr(1, 8))\n",
    "df = df.withColumn(\"end_time\", F.split(F.col(\"end_time\"), \"T\").getItem(1).substr(1, 8))\n",
    "\n",
    "# get start date id \n",
    "df_date = spark.read.format(\"delta\").table(table_date)\n",
    "join =    ((F.col(\"f.start_date\") == F.col(\"d.date\")))       \n",
    "df = (\n",
    "    df.alias(\"f\")\n",
    "        .join(\n",
    "            df_date.alias(\"d\"), join, \"left\"\n",
    "        ).select(\"f.*\", \n",
    "                F.col(\"d.date_id\").alias(\"start_date_id\"),        \n",
    ").drop(\"start_date\")\n",
    ")\n",
    "\n",
    "# get end date_id\n",
    "join =    ((F.col(\"f.end_date\") == F.col(\"d.date\")))       \n",
    "df = (\n",
    "    df.alias(\"f\")\n",
    "        .join(\n",
    "            df_date.alias(\"d\"), join, \"left\"\n",
    "        ).select(\"f.*\", \n",
    "                F.col(\"d.date_id\").alias(\"end_date_id\"),        \n",
    ").drop(\"end_date\")\n",
    ")\n",
    "\n",
    "# get start time id \n",
    "df_time = spark.read.format(\"delta\").table(table_time)\n",
    "join =    ((F.col(\"f.start_time\") == F.col(\"d.time_15min\")))       \n",
    "df = (\n",
    "    df.alias(\"f\")\n",
    "        .join(\n",
    "            df_time.alias(\"d\"), join, \"left\"\n",
    "        ).select(\"f.*\", \n",
    "                F.col(\"d.time_quarter_id\").alias(\"start_time_id\"),        \n",
    ").drop(\"start_time\")\n",
    ")\n",
    "\n",
    "# get end date_id\n",
    "join =    ((F.col(\"f.end_time\") == F.col(\"d.time_15min\")))       \n",
    "df = (\n",
    "    df.alias(\"f\")\n",
    "        .join(\n",
    "            df_time.alias(\"d\"), join, \"left\"\n",
    "        ).select(\"f.*\", \n",
    "                F.col(\"d.time_quarter_id\").alias(\"end_time_id\"),        \n",
    ").drop(\"end_time\")\n",
    ")\n",
    "\n",
    "df = df.withColumnRenamed(\"dataset_id\", \"source_dataset_id\")\n",
    "\n",
    "df = df.withColumn(\"refresh_timestamp\", F.current_timestamp())\n",
    "\n",
    "external_path = \"abfss://gold@fingridtest.dfs.core.windows.net/gold/fact_forecast/\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(external_path)\n",
    "\n",
    "\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d4b7fa99-82d2-4659-a09a-63f5d92c391a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS fingrid_test_workspace.fingrid_gold.fact_forecast\n",
    "USING DELTA\n",
    "LOCATION 'abfss://gold@fingridtest.dfs.core.windows.net/gold/fact_forecast/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb8a3ed-d711-44d6-a2f5-b5939b45de62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from fingrid_test_workspace.fingrid_gold.fact_forecast"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5167004270313714,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fact_forecast",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
