on:
  push:
    branches:
    - main

permissions:
      id-token: write
      contents: read

jobs:
  build:
    runs-on: ubuntu-latest
    steps:

    - uses: actions/checkout@v4
# Installs Node and the npm packages saved in your package.json file in the build
    - name: Setup Node.js environment
      uses: actions/setup-node@v4
      with:
        node-version: 18.x
        
    - name: install ADF Utilities package
      run: npm install
      working-directory: ${{github.workspace}}/adf/build  # (1) provide the folder location of the package.json file
        
# Validates all of the Data Factory resources in the repository. You'll get the same validation errors as when "Validate All" is selected.
    - name: Validate
      run: npm run build validate ${{github.workspace}}/adf /subscriptions/1cf6ebb2-6c97-4ec4-9b7a-22a94a4dafc8/resourceGroups/RG-Fingrid-test/providers/Microsoft.DataFactory/factories/Fingrid-test-001 # (2) The validate command needs the root folder location of your repository where all the objects are stored. And the 2nd parameter is the resourceID of the ADF instance 
      working-directory: ${{github.workspace}}/adf/build
 

    - name: Validate and Generate ARM template<
      run: npm run build export ${{github.workspace}}/adf /subscriptions/1cf6ebb2-6c97-4ec4-9b7a-22a94a4dafc8/resourceGroups/RG-Fingrid-test/providers/Microsoft.DataFactory/factories/Fingrid-test-001 "ExportedArmTemplate"  # (3) The build command, as validate, needs the root folder location of your repository where all the objects are stored. And the 2nd parameter is the resourceID of the ADF instance. The 3rd parameter is the exported ARM template artifact name 
      working-directory: ${{github.workspace}}/adf/build
 
# In order to leverage the artifact in another job, we need to upload it with the upload action 
    - name: upload artifact
      uses: actions/upload-artifact@v4
      with:
        name: ExportedArmTemplate # (4) use the same artifact name you used in the previous export step
        path: ${{github.workspace}}/adf/build/ExportedArmTemplate

  deply_to_dev:
    needs: build
    runs-on: ubuntu-latest
    environment: DEV
    steps:
    
 # we 1st download the previously uploaded artifact so we can leverage it later in the release job     
      - name: Download a Build Artifact
        uses: actions/download-artifact@v4
        with:
          name: ExportedArmTemplate # (5) Artifact name


      - name: Login via Az module
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          enable-AzPSSession: true 

      - name: data-factory-deploy
        uses: Azure/data-factory-deploy-action@v1.2.0
        with:
          resourceGroupName: RG-Fingrid-test
          dataFactoryName: Fingrid-test-001
          armTemplateFile: ARMTemplateForFactory.json
          armTemplateParametersFile: ARMTemplateParametersForFactory.json
          additionalParameters: >
            AzureDataLakeStorage1_properties_typeProperties_url=${{ vars.ADLSG2_URL}}
            AzureDatabricks1_properties_typeProperties_existingClusterId=${{ vars.EXISTINGCLUSTERID}}
            AzureDatabricks1_properties_typeProperties_domain=${{ vars.DATABRICKS_DOMAIN}}
            AzureDatabricks1_properties_typeProperties_workspaceResourceId=${{ vars.DATABRICKS_WORKRESOUCEID}}
            AzureDatabricksDeltaLake_properties_typeProperties_ClusterId=${{ vars.EXISTINGCLUSTERID}}
            AzureDatabricksDeltaLake_properties_typeProperties_domain=${{ vars.DELTALAKE_DOMAIN}}
            AzureDatabricksDeltaLake_properties_typeProperties_workspaceResourceId=${{ vars.DELTALAKE_WORKSPACERESOURCEID}}
            default_properties_catalog_value=${{ vars.CATALOG}}
            default_properties_load_control_schema_value=${{ vars.LOAD_CONTROL_SCHEMA}}


  deploy_to_prod:
    needs: deply_to_dev  
    runs-on: ubuntu-latest
    environment: PROD
    steps:
      - name: Download a Build Artifact
        uses: actions/download-artifact@v4
        with:
          name: ExportedArmTemplate
  
      - name: Login via Az module
        uses: azure/login@v2
        with:
          client-id: ${{ secrets.AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
          enable-AzPSSession: true
      - name: Validate parameters
        run: |
          echo "Checking secret existence:"
          echo "ADLS URL: ${{ vars.ADLSG2_URL_PROD != '' }}"
          echo "Cluster ID: ${{ vars.EXISTINGCLUSTERID_PROD != '' }}"

  
      - name: data-factory-deploy
        uses: Azure/data-factory-deploy-action@v1.2.0
        with:
          resourceGroupName: RG-Fingrid-Prod           
          dataFactoryName: Fingrid-Prod            
          armTemplateFile: ARMTemplateForFactory.json
          armTemplateParametersFile: ARMTemplateParametersForFactory.json
          additionalParameters: >
            AzureDataLakeStorage1_properties_typeProperties_url=${{ vars.ADLSG2_URL_PROD }}
            AzureDatabricks1_properties_typeProperties_existingClusterId=${{ vars.EXISTINGCLUSTERID_PROD }}
            AzureDatabricks1_properties_typeProperties_domain=${{ vars.DATABRICKS_DOMAIN_PROD }}
            AzureDatabricks1_properties_typeProperties_workspaceResourceId=${{ vars.DATABRICKS_WORKRESOUCEID_PROD }}
            AzureDatabricksDeltaLake_properties_typeProperties_domain=${{ vars.DELTALAKE_DOMAIN_PROD }}
            AzureDatabricksDeltaLake_properties_typeProperties_ClusterId=${{ vars.EXISTINGCLUSTERID_PROD}}
            AzureDatabricksDeltaLake_properties_typeProperties_workspaceResourceId=${{ vars.DELTALAKE_WORKSPACERESOURCEID_PROD }}
            default_properties_catalog_value=${{ vars.CATALOG_PROD }}
            default_properties_load_control_schema_value=${{ vars.LOAD_CONTROL_SCHEMA_PROD }}
  

          # skipAzModuleInstallation:  # Parameters which skip the Az module installation. Optional, default is false.      
